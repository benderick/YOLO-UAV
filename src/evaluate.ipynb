{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.yolo.detect.val import DetectionValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(\n",
    "    model = \"./tmp/best.pt\",\n",
    "    data = \"/home/futurama/zhangshuo/YOLO-UAV/data/VisDrone/VisDrone.yaml\",\n",
    "    conf=0.001,\n",
    "    batch=1,\n",
    "    device=\"cuda:0\",\n",
    "    save_json=True\n",
    "    )\n",
    "\n",
    "save_dir = Path(\"./logs/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = DetectionValidator(save_dir=save_dir, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.88 ðŸš€ Python-3.10.16 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "YOLO11s summary (fused): 100 layers, 9,416,670 parameters, 0 gradients, 21.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”” val: Scanning /zhangshuo/data/VisDrone/VisDrone2019-DET-val/labels.cache... 548 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548/548 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548/548 [00:27<00:00, 20.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        548      38759       0.47      0.355      0.361      0.217\n",
      "            pedestrian        520       8844      0.465      0.381      0.385      0.171\n",
      "                people        482       5125      0.544      0.252      0.306      0.116\n",
      "               bicycle        364       1287      0.242      0.151      0.112     0.0458\n",
      "                   car        515      14064       0.64      0.763      0.766      0.545\n",
      "                   van        421       1975      0.536      0.424      0.443      0.309\n",
      "                 truck        266        750      0.464      0.308      0.322       0.22\n",
      "              tricycle        337       1045      0.442      0.215      0.239      0.135\n",
      "       awning-tricycle        220        532      0.343      0.147      0.154      0.095\n",
      "                   bus        131        251      0.539      0.532      0.504      0.366\n",
      "                 motor        485       4886      0.485       0.38       0.38      0.169\n",
      "Speed: 1.1ms preprocess, 22.3ms inference, 0.0ms loss, 3.3ms postprocess per image\n",
      "ðŸ”” Using pycocotools:\n",
      "Saving val_pred_coco.json\n",
      "Evaluating pycocotools mAP using val_pred_coco.json and /zhangshuo/data/VisDrone/annotations/val_coco.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.66s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=1.53s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=40.34s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=2.13s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.192\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.334\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.190\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.305\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.441\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.093\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.245\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.212\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.472\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.584\n",
      "\n",
      "COCOè¯„ä¼°ç»“æžœ:\n",
      "      AP     AP50     AP75      APs      APm      APl\n",
      "   0.192    0.334    0.190    0.090    0.305    0.441\n",
      "\n",
      "     AR1     AR10    AR100      ARs      ARm      ARl\n",
      "   0.093    0.245    0.321    0.212    0.472    0.584\n",
      "\n",
      "Results saved to ðŸ”” .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metrics/precision(B)': np.float64(0.47011312992313165),\n",
       " 'metrics/recall(B)': np.float64(0.3552943449756788),\n",
       " 'metrics/mAP50(B)': np.float64(0.3342522983143088),\n",
       " 'metrics/mAP50-95(B)': np.float64(0.19176865235340967),\n",
       " 'fitness': np.float64(0.2316585844724985)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
