{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "import torch, yaml, cv2, os, shutil\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from PIL import Image\n",
    "from ultralytics.nn.tasks import DetectionModel as Model\n",
    "from ultralytics.utils.torch_utils import intersect_dicts\n",
    "from ultralytics.utils.ops import xywh2xyxy\n",
    "from pytorch_grad_cam import GradCAMPlusPlus, GradCAM, XGradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, ratio, (dw, dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yolo_heatmap:\n",
    "    def __init__(self, weight, cfg, device, method, layer, backward_type, conf_threshold, ratio):\n",
    "        device = torch.device(device)\n",
    "        ckpt = torch.load(weight)\n",
    "        model_names = ckpt['model'].names\n",
    "        csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\n",
    "        model = Model(cfg, ch=3, nc=len(model_names)).to(device)\n",
    "        csd = intersect_dicts(csd, model.state_dict(), exclude=['anchor'])  # intersect\n",
    "        model.load_state_dict(csd, strict=False)  # load\n",
    "        model.eval()\n",
    "        print(f'Transferred {len(csd)}/{len(model.state_dict())} items')\n",
    "\n",
    "        target_layers = [eval(layer)]\n",
    "        method = eval(method)\n",
    "\n",
    "        colors = np.random.uniform(0, 255, size=(len(model_names), 3)).astype(np.int32)\n",
    "        self.__dict__.update(locals())\n",
    "\n",
    "    def post_process(self, result):\n",
    "        logits_ = result[:, 4:]\n",
    "        boxes_ = result[:, :4]\n",
    "        sorted, indices = torch.sort(logits_.max(1)[0], descending=True)\n",
    "        return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[\n",
    "            indices[0]], xywh2xyxy(torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]]).cpu().detach().numpy()\n",
    "\n",
    "    def draw_detections(self, box, color, name, img):\n",
    "        xmin, ymin, xmax, ymax = list(map(int, list(box)))\n",
    "        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), tuple(int(x) for x in color), 2)\n",
    "        cv2.putText(img, str(name), (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, tuple(int(x) for x in color), 2,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "        return img\n",
    "\n",
    "    def __call__(self, img_path):\n",
    "        # img process\n",
    "        img = cv2.imread(img_path)\n",
    "        img = letterbox(img)[0]\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = np.float32(img) / 255.0\n",
    "        tensor = torch.from_numpy(np.transpose(img, axes=[2, 0, 1])).unsqueeze(0).to(self.device)\n",
    "\n",
    "        # init ActivationsAndGradients\n",
    "        grads = ActivationsAndGradients(self.model, self.target_layers, reshape_transform=None)\n",
    "\n",
    "        # get ActivationsAndResult\n",
    "        result = grads(tensor)\n",
    "        activations = grads.activations[0].cpu().detach().numpy()\n",
    "\n",
    "        # postprocess to yolo output\n",
    "        post_result, pre_post_boxes, post_boxes = self.post_process(result[0])\n",
    "        for i in trange(int(post_result.size(0) * self.ratio)):\n",
    "            if float(post_result[i].max()) < self.conf_threshold:\n",
    "                break\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            # get max probability for this prediction\n",
    "            if self.backward_type == 'class' or self.backward_type == 'all':\n",
    "                score = post_result[i].max()\n",
    "                score.backward(retain_graph=True)\n",
    "\n",
    "            if self.backward_type == 'box' or self.backward_type == 'all':\n",
    "                for j in range(4):\n",
    "                    score = pre_post_boxes[i, j]\n",
    "                    score.backward(retain_graph=True)\n",
    "\n",
    "            # process heatmap\n",
    "            if self.backward_type == 'class':\n",
    "                gradients = grads.gradients[0]\n",
    "            elif self.backward_type == 'box':\n",
    "                gradients = grads.gradients[0] + grads.gradients[1] + grads.gradients[2] + grads.gradients[3]\n",
    "            else:\n",
    "                gradients = grads.gradients[0] + grads.gradients[1] + grads.gradients[2] + grads.gradients[3] + \\\n",
    "                            grads.gradients[4]\n",
    "            b, k, u, v = gradients.size()\n",
    "            weights = self.method.get_cam_weights(self.method, None, None, None, activations,\n",
    "                                                  gradients.detach().numpy())\n",
    "            weights = weights.reshape((b, k, 1, 1))\n",
    "            saliency_map = np.sum(weights * activations, axis=1)\n",
    "            saliency_map = np.squeeze(np.maximum(saliency_map, 0))\n",
    "            saliency_map = cv2.resize(saliency_map, (tensor.size(3), tensor.size(2)))\n",
    "            saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
    "            if (saliency_map_max - saliency_map_min) == 0:\n",
    "                continue\n",
    "            saliency_map = (saliency_map - saliency_map_min) / (saliency_map_max - saliency_map_min)\n",
    "\n",
    "            # add heatmap and box to image\n",
    "            cam_image = show_cam_on_image(img.copy(), saliency_map, use_rgb=True)\n",
    "            \"不想在图片中绘画出边界框和置信度，注释下面的一行代码即可\"\n",
    "            cam_image = self.draw_detections(post_boxes[i], self.colors[int(post_result[i, :].argmax())],\n",
    "                                             f'{self.model_names[int(post_result[i, :].argmax())]} {float(post_result[i].max()):.2f}',\n",
    "                                             cam_image)\n",
    "            cam_image = Image.fromarray(cam_image)\n",
    "            cam_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "    params = {\n",
    "        'weight': '/home/futurama/zhangshuo/YOLO-UAV/src/bestt.pt',  # 训练出来的权重文件\n",
    "        'cfg': '/home/futurama/zhangshuo/YOLO-UAV/configs/model/yolo11.yaml',  # 训练权重对应的yaml配置文件\n",
    "        'device': 'cuda:0',\n",
    "        'method': 'GradCAM',  # GradCAMPlusPlus, GradCAM, XGradCAM , 使用的热力图库文件不同的效果不一样可以多尝试\n",
    "        'layer': 'model.model[9]',  # 想要检测的对应层\n",
    "        'backward_type': 'all',  # class, box, all\n",
    "        'conf_threshold': 0.01,  # 0.6  # 置信度阈值，有的时候你的进度条到一半就停止了就是因为没有高于此值的了\n",
    "        'ratio': 0.02  # 0.02-0.1\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = yolo_heatmap(**get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(r'/home/futurama/zhangshuo/YOLO-UAV/data/VisDrone/VisDrone2019-DET-test-dev/images/0000006_00159_d_0000001.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
